# WEATHER PREDICTION CASE STUDY
<br>
Currently, the internet is so full of content and it's infeasible to evaluate things manually. Our main task would be to predict the weather by analyzing data and categorizing them into certain labels. Such a problem can be solved using various Machine learning techniques, and that’s what we are going to do here. We will then compare the results using some evaluation metrics to understand why it fits the best. There is no fun until it is accessible to every person out there that's why we are here to deploy it too.
<br>
<br>
# WHY MACHINE-LEARNING?
<br>
The purpose of machine learning is to discover patterns in your data and then make predictions based on often complex patterns to answer business questions, detect and analyze trends and help solve problems.
Discovering patterns and Analyzing data is a task that can also be performed by humans manually but given large datasets and many decision making features we need to SCALE UP!!
Here Machine learning comes into the picture, which can be used to easily scale up over large data and features to discover complex patterns in the datasets among the given features and make future predictions by learning these patterns.

<br>
<br>
#Machine learning pipeline
COLLECT>>STORE>>ENRICH>>TRAIN/APPLY>>VISUALIZE

1.Collect : - First and the most important step is to collect data coz without it we can’t proceed further. We need a sufficient  quantity of  data so that our model can be trained such that it makes accurate predictions. Nowadays techniques like web-scraping are used to extract data from web and open-source websites like kaggle provides some general purpose datasets for free.

2.Store- After collecting data we need to store that data such that it is machine readable.Eg. We use .csv files in general to read data from as python libraries provide functions that can parse and read csv files automatically.

3.Enrich-  The data we get can have following problems at some points or columns: incomplete,not-scaled,deviating,non-numeric etc.  To brush-up the data we use the Pandas library in python so that our machine learning model does not get confused by some data inaccuracies.

4.Train/Test - Perhaps the heart of the machine learning pipeline is this step where the actual learning takes place where our machine learning model learns from the data given to it. First we split the data into train-data(given to a machine learning model to train) and test-data(used to evaluate our model accuracy). After that the train data is passed on to a machine learning algorithm where actually the machine learns from our data by using mathematics to find patterns in the data.

5.Visualize - After our model is ready it is the time we test our model. The data predictions made by our model are generally numbers which are very hard to visualize and ponder upon. So we plot these predictions on different kinds of graphs to visualize future data trends in a clear manner. We will be using matplotlib/seaborn for data visualization.
  
<br>
<br>
#Tools and Technologies:
Python
Sklearn
Jupyter notebook / google colaboratory
Numpy, pandas
Tensorflow
<br>
<br>
1.DATA MANAGEMENT
  PANDAS 
  NUMPY

<br>
<br>
2.DATA VISUALIZATION
 PYTHON-MATPLOTLIB
 SEABORN
<br>
<br>
 3.Models:
  LOGISTIC REGRESSION
  NAIVE BAYE
  Artificial Neural Networks (ANN)
  KNN
 
 <br>
 <br>
 
4.Deployment:
 Flask + Html + Css
 Tkinter

#PROJECT IN 4 PARTS:
<br>
Getting started with Data Handling and Visualization with Pandas and Matplotlib/Seaborn.
<br>
Understanding feature importance and trying different methods. Implementing two basic models on real world data.
<br>
Implementing two more advanced algorithms and comparing their performances.
Deployment of model once ready.

